{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Quarter', 'Consumption', 'Income', 'Production', 'Savings',\n",
      "       'Unemployment'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Consumption</th>\n",
       "      <th>Income</th>\n",
       "      <th>Production</th>\n",
       "      <th>Savings</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970 Q1</td>\n",
       "      <td>0.618566</td>\n",
       "      <td>1.044801</td>\n",
       "      <td>-2.452486</td>\n",
       "      <td>5.299014</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970 Q2</td>\n",
       "      <td>0.451984</td>\n",
       "      <td>1.225647</td>\n",
       "      <td>-0.551459</td>\n",
       "      <td>7.789894</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970 Q3</td>\n",
       "      <td>0.872872</td>\n",
       "      <td>1.585154</td>\n",
       "      <td>-0.358652</td>\n",
       "      <td>7.403984</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970 Q4</td>\n",
       "      <td>-0.271848</td>\n",
       "      <td>-0.239545</td>\n",
       "      <td>-2.185691</td>\n",
       "      <td>1.169898</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1971 Q1</td>\n",
       "      <td>1.901345</td>\n",
       "      <td>1.975925</td>\n",
       "      <td>1.909764</td>\n",
       "      <td>3.535667</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2018 Q2</td>\n",
       "      <td>0.983112</td>\n",
       "      <td>0.661825</td>\n",
       "      <td>1.117424</td>\n",
       "      <td>-2.723974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2018 Q3</td>\n",
       "      <td>0.853181</td>\n",
       "      <td>0.806271</td>\n",
       "      <td>1.256722</td>\n",
       "      <td>-0.085686</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>2018 Q4</td>\n",
       "      <td>0.356512</td>\n",
       "      <td>0.695142</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>5.031337</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2019 Q1</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>1.100753</td>\n",
       "      <td>-0.488206</td>\n",
       "      <td>9.760287</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2019 Q2</td>\n",
       "      <td>1.113517</td>\n",
       "      <td>0.593399</td>\n",
       "      <td>-0.539949</td>\n",
       "      <td>-4.264616</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Quarter  Consumption    Income  Production   Savings  Unemployment\n",
       "1    1970 Q1     0.618566  1.044801   -2.452486  5.299014           0.9\n",
       "2    1970 Q2     0.451984  1.225647   -0.551459  7.789894           0.5\n",
       "3    1970 Q3     0.872872  1.585154   -0.358652  7.403984           0.5\n",
       "4    1970 Q4    -0.271848 -0.239545   -2.185691  1.169898           0.7\n",
       "5    1971 Q1     1.901345  1.975925    1.909764  3.535667          -0.1\n",
       "..       ...          ...       ...         ...       ...           ...\n",
       "194  2018 Q2     0.983112  0.661825    1.117424 -2.723974           0.0\n",
       "195  2018 Q3     0.853181  0.806271    1.256722 -0.085686          -0.3\n",
       "196  2018 Q4     0.356512  0.695142    0.948148  5.031337           0.2\n",
       "197  2019 Q1     0.282885  1.100753   -0.488206  9.760287          -0.1\n",
       "198  2019 Q2     1.113517  0.593399   -0.539949 -4.264616          -0.1\n",
       "\n",
       "[198 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(30)\n",
    "\n",
    "df = pd.read_csv(\"data/us_change.csv\", index_col=0)\n",
    "print(df.columns)  # Lista todas as colunas disponíveis\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_period(quarter):\n",
    "    \"\"\"\n",
    "    Function to convert a string to a pandas period object. \n",
    "    Checks if the input is a string in the form '2019 Q1' or already a Period.\n",
    "    \"\"\"\n",
    "    if isinstance(quarter, pd.Period):\n",
    "        return quarter\n",
    "    year, quarter_str = quarter.split()\n",
    "    year = int(year)\n",
    "    quarter = int(quarter_str[-1])\n",
    "    return pd.Period(year=year, quarter=quarter, freq='Q')\n",
    "\n",
    "df['Quarter'] = df['Quarter'].apply(convert_to_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 5s 94ms/step - loss: 0.3313 - val_loss: 0.1153\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0692 - val_loss: 0.0244\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0387 - val_loss: 0.0054\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.0069\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.0067\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0230 - val_loss: 0.0052\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0217 - val_loss: 0.0051\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0215 - val_loss: 0.0051\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0216 - val_loss: 0.0051\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.0057\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0049\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0230 - val_loss: 0.0063\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0048\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0059\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0213 - val_loss: 0.0049\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0048\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0213 - val_loss: 0.0055\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0210 - val_loss: 0.0047\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0061\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0218 - val_loss: 0.0047\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.0047\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0213 - val_loss: 0.0059\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0047\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0211 - val_loss: 0.0055\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0212 - val_loss: 0.0047\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0213 - val_loss: 0.0053\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0207 - val_loss: 0.0046\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0210 - val_loss: 0.0050\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0207 - val_loss: 0.0048\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0208 - val_loss: 0.0047\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0207 - val_loss: 0.0048\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0205 - val_loss: 0.0047\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.0046\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0203 - val_loss: 0.0051\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0204 - val_loss: 0.0047\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0211 - val_loss: 0.0058\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0220 - val_loss: 0.0052\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0203 - val_loss: 0.0048\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0047\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.0202 - val_loss: 0.0047\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0200 - val_loss: 0.0046\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0199 - val_loss: 0.0049\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0203 - val_loss: 0.0046\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0206 - val_loss: 0.0049\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0204 - val_loss: 0.0055\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0208 - val_loss: 0.0054\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0213 - val_loss: 0.0050\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0205 - val_loss: 0.0048\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0206 - val_loss: 0.0046\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004648719914257526"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Simulando o DataFrame com dados fictícios\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df.drop(columns=[\"Quarter\"]))\n",
    "\n",
    "# Criando a função para gerar as sequências para a LSTM\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length, :-1])\n",
    "        labels.append(data[i + sequence_length, 0])  # \"Consumption\" como target\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Definindo os parâmetros\n",
    "sequence_length = 5  # Número de passos no histórico\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Construindo o modelo LSTM\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dense(25, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mean_squared_error\")\n",
    "\n",
    "# Treinando o modelo\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Avaliando o modelo no conjunto de teste\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 4ms/step\n",
      "Mean Absolute Error (MAE) na escala original: 0.12019681245042772\n",
      "Root Mean Squared Error (RMSE) na escala original: 0.15681750748948123\n",
      "Mean Absolute Scaled Error (MASE): 0.3987874387362746\n",
      "R²: 0.009399558478579206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def mean_absolute_scaled_error(y_true, y_pred, y_train):\n",
    "    naive_forecast_errors = np.abs(np.diff(y_train))  # Diferença do forecast ingênuo\n",
    "    mae_naive = np.mean(naive_forecast_errors)  # MAE do modelo ingênuo\n",
    "    mae_model = mean_absolute_error(y_true, y_pred)  # MAE do modelo\n",
    "    return mae_model / mae_naive\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# aqui estamos revertendo a escala porque a utilizamos MinMaxScaler na LSTM\n",
    "y_test_original = scaler.inverse_transform(np.hstack([np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), y_test.reshape(-1, 1)]))[:, -1]\n",
    "y_pred_original = scaler.inverse_transform(np.hstack([np.zeros((y_pred.shape[0], scaled_data.shape[1] - 1)), y_pred]))[:, -1]\n",
    "\n",
    "\n",
    "mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse_original = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "mase_original = mean_absolute_scaled_error(y_test_original, y_pred_original, scaler.inverse_transform(np.hstack([np.zeros((y_train.shape[0], scaled_data.shape[1] - 1)), y_train.reshape(-1, 1)]))[:, -1])\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE) na escala original: {mae_original}\")\n",
    "print(f\"Root Mean Squared Error (RMSE) na escala original: {rmse_original}\")\n",
    "print(f\"Mean Absolute Scaled Error (MASE): {mase_original}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotando os resultados:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
